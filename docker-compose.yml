---
version: '3'
services:
  zeppelin:
    hostname: zeppelin
    container_name: zeppelin
    image: apache/zeppelin:0.8.2
    depends_on:
    - neo4j
    ports:
      - 8080:8080
    environment: 
      ZEPPELIN_ADDR: 0.0.0.0
    volumes:
      - ./zeppelin/notebook:/zeppelin/notebook
      - ./zeppelin/interpreter/neo4j:/zeppelin/interpreter/neo4j
      - ./zeppelin/conf:/zeppelin/conf
      - ./zeppelin/data:/zeppelin/spark-warehouse

  neo4j:
    image: neo4j:4.1.1-enterprise
    hostname: neo4j
    container_name: neo4j
    ports:
    - "7474:7474"
    - "7687:7687"
    depends_on:
    - zookeeper
    - broker
    environment:
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_AUTH: neo4j/streams
      NEO4J_dbms_memory_heap_max__size: 2G
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*,gds.*"
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_kafka_zookeeper_connect: zookeeper:2181
      NEO4J_kafka_bootstrap_servers: broker:9093
      NEO4J_streams_source_schema_polling_interval: 1000
      NEO4J_streams_sink_enabled: "true"
      # Test Source Basket
      NEO4J_streams_source_topic_nodes_purchases_from_basket: Customer{id,name,surname};Product{sku,name}
      NEO4J_streams_source_topic_relationships_purchases_from_basket: BOUGHT{quantity}
      # Test Sink Cypher Template
      NEO4J_streams_sink_topic_cypher_knows: "MERGE (source:Person{id: event.source.id}) SET source += event.source.properties
        MERGE (target:Person{id: event.target.id}) SET target += event.target.properties
        MERGE (source)-[know:KNOWS]->(target) SET know += event.properties"
      # Test Sink CDC Schema
      NEO4J_streams_source_topic_nodes_cdcschematopic_from_cdcschemasource: Person{*};Movie{*}
      NEO4J_streams_source_topic_relationships_cdcschematopic_from_cdcschemasource: ACTED_IN{*};DIRECTED{*};PRODUCED{*};WROTE{*};FOLLOWS{*};REVIEWED{*}
      NEO4J_streams_sink_topic_cdc_schema_to_cdcschematarget: cdcschematopic
      # Test JSON projection
      NEO4J_streams_sink_topic_pattern_node_jsonprojuser_to_jsonprojection: User{!userId}
      NEO4J_streams_sink_topic_pattern_node_jsonprojproduct_to_jsonprojection: Product{!productId}
      NEO4J_streams_sink_topic_pattern_relationship_jsonprojbought_to_jsonprojection: (:User:Customer{!userId})-[:BOUGHT]->(:Product{!productId})
      # Test CUD file
      NEO4J_streams_sink_topic_cud_to_cud: cudtopic
      # DLQ configuration
      NEO4K_streams_sink_errors_tolerance: "all"
      NEO4K_streams_sink_errors_log_enable: "true"
      NEO4K_streams_sink_errors_deadletterqueue_topic_name: "dlqtopic"
      NEO4K_streams_sink_errors_deadletterqueue_context_headers_enable: "true"
      NEO4K_streams_sink_errors_deadletterqueue_context_headers_prefix: "__streams.errors."
    volumes:
    - ./neo4j/plugins:/plugins
    # - ./neo4j/data:/data

  zookeeper:
    image: confluentinc/cp-zookeeper
    hostname: zookeeper
    container_name: zookeeper
    ports:
    - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-enterprise-kafka
    hostname: broker
    container_name: broker
    depends_on:
    - zookeeper
    ports:
    - "9092:9092"
    expose:
    - "9093"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:9093
      # workaround if we change to a custom name the schema_registry fails to start
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      #
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'

  # schema_registry:
  #   image: confluentinc/cp-schema-registry
  #   hostname: schema_registry
  #   container_name: schema_registry
  #   depends_on:
  #   - zookeeper
  #   - broker
  #   ports:
  #   - "8081:8081"
  #   environment:
  #     SCHEMA_REGISTRY_HOST_NAME: schema_registry
  #     SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'

  # connect:
  #   image: confluentinc/cp-kafka-connect
  #   hostname: connect
  #   container_name: connect
  #   depends_on:
  #   - zookeeper
  #   - broker
  #   # - schema_registry
  #   ports:
  #   - "8083:8083"
  #   environment:
  #     CONNECT_BOOTSTRAP_SERVERS: 'broker:9093'
  #     CONNECT_REST_ADVERTISED_HOST_NAME: connect
  #     CONNECT_REST_PORT: 8083
  #     CONNECT_GROUP_ID: compose-connect-group
  #     CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
  #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
  #     CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
  #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
  #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
  #     CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema_registry:8081'
  #     CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
  #     CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema_registry:8081'
  #     CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
  #     CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
  #     CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
  #     CONNECT_PLUGIN_PATH: /usr/share/java,/tmp/connect-plugins,/usr/share/confluent-hub-components
  #     CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=DEBUG,org.I0Itec.zkclient=DEBUG,org.reflections=ERROR,org.apache.kafka.connect.transforms
  #   command: 
  #     - bash 
  #     - -c 
  #     - |
  #       confluent-hub install --no-prompt neo4j/kafka-connect-neo4j:1.0.9
  #       confluent-hub install --no-prompt confluentinc/connect-transforms:latest
  #       /etc/confluent/docker/run

  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:6.7.0
  #   container_name: elasticsearch
  #   hostname: elasticsearch
  #   ports:
  #     - 9200:9200
  #   environment:
  #     ES_JAVA_OPTS: "-Xms1g -Xmx1g"

  # kibana:
  #   image: docker.elastic.co/kibana/kibana:6.7.0
  #   container_name: kibana
  #   hostname: kibana
  #   depends_on:
  #     - elasticsearch
  #   ports:
  #     - 5601:5601
  #   command: 
  #     - bash 
  #     - -c 
  #     - |
  #       /usr/local/bin/kibana-docker &
  #       echo "Waiting for Kibana to be ready (checking http://localhost:5601/api/status) ‚è≥"
  #       while [ $$(curl -s http://localhost:5601/api/status|grep --silent -e "\"status\":{\"overall\":{\"state\":\"green\"";echo $$?) -eq 1 ] ; do 
  #       sleep 5 
  #       done
  #       echo "Pre-creating movies index pattern"
  #       curl -XPOST 'http://localhost:5601/api/saved_objects/index-pattern/movie' \
  #         -H 'kbn-xsrf: nevergonnagiveyouup' \
  #         -H 'Content-Type: application/json' \
  #         -d '{"attributes":{"title":"movie"}}'
  #       echo "Pre-creating person index pattern"
  #       curl -XPOST 'http://localhost:5601/api/saved_objects/index-pattern/person' \
  #         -H 'kbn-xsrf: nevergonnagiveyouup' \
  #         -H 'Content-Type: application/json' \
  #         -d '{"attributes":{"title":"person"}}'
  #       echo "Setting the person index pattern as default"
  #       curl -XPOST 'http://localhost:5601/api/kibana/settings' \
  #         -H 'kbn-xsrf: nevergonnagiveyouup' \
  #         -H 'content-type: application/json' \
  #         -d '{"changes":{"defaultIndex":"person"}}'
  #       echo "Opting out of telemetry"
  #       curl -XPOST 'http://localhost:5601/api/telemetry/v1/optIn' \
  #         -H 'kbn-xsrf: nevergonnagiveyouup' \
  #         -H 'content-type: application/json' \
  #         -d '{"enabled":false}'
  #       sleep infinity